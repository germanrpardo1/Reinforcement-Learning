{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "95cdcfa0",
   "metadata": {},
   "source": [
    "### In this homework assigment, you are required to apply the neural fitted Q-iteration algorithm to a pre-collected dataset for batch (offline) policy optimisation. Please follow the instructions detailed below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "875d34bd",
   "metadata": {},
   "source": [
    "Step 1: Generate an offline dataset. Consider the CartPole example. We will use a sub-optimal policy for data generation. Specifically, consider the following deterministic policy $\\pi_b$ that returns 0 (left) if the pole angle is negative and 1 otherwise. To allow exploration, we set the behavior policy to be a mixture of $\\pi_b$ and a uniform random policy. Specifically, the agent will follow the uniform random policy or $\\pi_b$ with equal probability. We simulate 100 episodes under this policy. This yields the offline dataset.\n",
    "\n",
    "Step 2: Fitted Q-iteration. We will apply the neural fitted Q-iteration (FQI) algorithm to this offline data to compute an optimal policy with three different choices of $\\gamma$, corresponding to 0.95, 0.99 and 1. Please refer to Page 43 of Lecture 5 for the pseudocode of FQI in batch settings. We repeat the Q-iteration 20 times, e.g., apply supervised learning algorithms 20 times to learn the optimal Q-function. The initial Q-estimator can be set to a zero function. Each iteration yields a Q-estimator, based on which we can derive an estimated optimal policy. In total, we obtain 20 $\\times$ 3 (3 choices of $\\gamma$) different policies.\n",
    "\n",
    "* To combine FQI with neural networks, we consider using the [MLPregressor](https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPRegressor.html) function. We can use the default neural network architecture (no need to specify no. of layers or no. of hidden nodes per layer). We may set the maximum number of iterations to 500.\n",
    "\n",
    "* In this example, we only have two actions (either pushing the cart to the left or to the right). As such, it would be better to use the second type of value function approximators on Page 11 of Lecture 5 (e.g., for each action, use a separate model for the value). The last type of approximators would be preferred in settings where we have a large action space.\n",
    "\n",
    "* The TD target depends on whether the current state is a terminal state or not. For a nonterminal state, the TD target is constructed as in the lecture slide. For a terminal state, the TD target is equal to the immediate reward.\n",
    "\n",
    "Step 3: Policy evaluation. For each of the computed 60 policies, we use the Monte Carlo method to evaluate the expected return under this policy, by generating 1000 episodes. Finally, plot all the returns in a single figure and comment on the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4723438a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neccesary packages for the implementation\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "\n",
    "# Imports enviroment CartPole from Gymnasium\n",
    "env = gym.make('CartPole-v1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd415a87",
   "metadata": {},
   "source": [
    "From seminar code, some description of the environment:\n",
    "\n",
    "#### Action\n",
    "Two discrete actions: a=0 indicates pushing cart to the left and a=1 pushes the cart to the right.  The amount the velocity reduced or increased does not only depends on the direction you are moving but also on the angle the pole is pointing. \n",
    "\n",
    "#### Reward\n",
    " Reward is +1 for every step taken, including the termination step.\n",
    " \n",
    "#### Observation\n",
    "There are 4 observations returned by the environment after each action taken by an agent:\n",
    "- Cart position:  a number between `-4.8` and `4.8`\n",
    "- Cart velocity: a number between `-inf`and `inf`\n",
    "- Pole angle: an angle between -24&deg; and 24&deg; \n",
    "-  Pole velocity at tip: a number between `-inf`and `inf`\n",
    "\n",
    "#### Termination\n",
    "- Cart position is smaller or greater than `-2.4` or `2.4`\n",
    "- Pole Angle is smaller or greater than -12&deg; or 12&deg;\n",
    "- Episode length is longer than 200"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c224b0c8",
   "metadata": {},
   "source": [
    "### Step 1\n",
    "Generate an offline dataset. Consider the CartPole example. We will use a sub-optimal policy for data generation. Specifically, consider the following deterministic policy $\\pi_b$ that returns 0 (left) if the pole angle is negative and 1 otherwise. To allow exploration, we set the behavior policy to be a mixture of $\\pi_b$ and a uniform random policy. Specifically, the agent will follow the uniform random policy or $\\pi_b$ with equal probability. We simulate 100 episodes under this policy. This yields the offline dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5c24f160",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Policy \\pi_b\n",
    "def action(angle):\n",
    "    if angle < 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c2d1308",
   "metadata": {},
   "outputs": [],
   "source": [
    "# D is where I will save all the observations for the training of the NNs (offline batch)\n",
    "# Each key represent an episode. \n",
    "# Saves observations  of the environment in the form (S_t, a_t, S_{t+1}, done)\n",
    "# It is not neccesary to save the reward since this is always 1\n",
    "def gen_data(D, episodes):\n",
    "    for i in range(episodes):\n",
    "        D[i] = []\n",
    "        S = env.reset()[0]\n",
    "        a = action(S[2])\n",
    "        done = False\n",
    "        while not done:\n",
    "            # Takes a step\n",
    "            next_S, r, done, info, _ = env.step(a)\n",
    "            \n",
    "            # Saves observations  of the environment in the form (S_t, a_t, S_{t+1}, done)\n",
    "            D[i].append([S, a, next_S, done])\n",
    "            \n",
    "            # Toss a coin: follow deterministic policy \\pi_b or random action is selected\n",
    "            if np.random.binomial(1, p=0.5) == 1:\n",
    "                # Action from policy \\pi_b\n",
    "                a = action(next_S[2])\n",
    "            else:\n",
    "                # Random action\n",
    "                a = env.action_space.sample()\n",
    "            S = next_S\n",
    "    return D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8b0c7695",
   "metadata": {},
   "outputs": [],
   "source": [
    "episodes = 100\n",
    "D = {}\n",
    "D = gen_data(D, episodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5277b34",
   "metadata": {},
   "source": [
    "### Step 2\n",
    "Fitted Q-iteration. We will apply the neural fitted Q-iteration (FQI) algorithm to this offline data to compute an optimal policy with three different choices of $\\gamma$, corresponding to 0.95, 0.99 and 1. Please refer to Page 43 of Lecture 5 for the pseudocode of FQI in batch settings. We repeat the Q-iteration 20 times, e.g., apply supervised learning algorithms 20 times to learn the optimal Q-function. The initial Q-estimator can be set to a zero function. Each iteration yields a Q-estimator, based on which we can derive an estimated optimal policy. In total, we obtain 20 $\\times$ 3 (3 choices of $\\gamma$) different policies.\n",
    "\n",
    "* To combine FQI with neural networks, we consider using the [MLPregressor](https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPRegressor.html) function. We can use the default neural network architecture (no need to specify no. of layers or no. of hidden nodes per layer). We may set the maximum number of iterations to 500.\n",
    "\n",
    "* In this example, we only have two actions (either pushing the cart to the left or to the right). As such, it would be better to use the second type of value function approximators on Page 11 of Lecture 5 (e.g., for each action, use a separate model for the value). The last type of approximators would be preferred in settings where we have a large action space.\n",
    "\n",
    "* The TD target depends on whether the current state is a terminal state or not. For a nonterminal state, the TD target is constructed as in the lecture slide. For a terminal state, the TD target is equal to the immediate reward.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c2ab788",
   "metadata": {},
   "source": [
    "#### Initialisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dec6bdaa",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def initialisation():\n",
    "    # This is the input of the neural network of the form (S_t, a_t)\n",
    "    # Recall the S_t is conformed by physical states (velocities and angles)\n",
    "    input0 = []\n",
    "    input1 = []\n",
    "\n",
    "    # Tuples of the next state\n",
    "    next_S0 = []\n",
    "    next_S1 = []\n",
    "    S = []\n",
    "\n",
    "    # Target\n",
    "    y0 = []\n",
    "    y1 = []\n",
    "\n",
    "    # Wheter state is terminal or not\n",
    "    done0 = []\n",
    "    done1 = []\n",
    "\n",
    "    for ep in range(episodes):\n",
    "        for step in range(len(D[ep])):\n",
    "            # Saves tuple for next state\n",
    "            S.append(D[ep][step][0])\n",
    "\n",
    "            # If action is 1, saves all the training input and output for this action\n",
    "            if D[ep][step][1] == 1:\n",
    "                input1.append(D[ep][step][0])\n",
    "                y1.append(1)\n",
    "                next_S1.append(D[ep][step][2])\n",
    "                done1.append(D[ep][step][3])\n",
    "\n",
    "            # If action is 0, saves all the training input and output for this action\n",
    "            else:\n",
    "                input0.append(D[ep][step][0])\n",
    "                y0.append(1)\n",
    "                next_S0.append(D[ep][step][2])\n",
    "                done0.append(D[ep][step][3])\n",
    "\n",
    "    # Training of 2 NNs\n",
    "    regr0 = MLPRegressor(random_state=1, max_iter=500).fit(input0, y0)\n",
    "    regr1 = MLPRegressor(random_state=1, max_iter=500).fit(input1, y1)\n",
    "\n",
    "    # Prediction using 2 NNs just trained\n",
    "    Q0_hat = regr0.predict(next_S0)\n",
    "    Q1_hat = regr1.predict(next_S1)\n",
    "\n",
    "    return input0, input1, Q0_hat, Q1_hat, next_S0, next_S1, S, done0, done1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0b4de8ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculates the TD target \n",
    "def calc_target(gamma, Q_hat, done):\n",
    "    y = []\n",
    "    for i in range(len(Q_hat)):\n",
    "        # If terminal state, target is 1\n",
    "        if done[i]:\n",
    "            y.append(1)\n",
    "        # If non terminal state, target computed as in lecture slides\n",
    "        else:\n",
    "            y.append(1 + gamma * Q_hat[i])\n",
    "    return y\n",
    "\n",
    "# Computes the maximum between the two Qfunction estimators\n",
    "def compute_max(Q0_final, Q1_final, S):\n",
    "    Q = {}\n",
    "    for i in range(len(Q0_final)):\n",
    "        if Q0_final[i] > Q1_final[i]:\n",
    "            Q[tuple(S[i]), 0] = Q0_final[i]\n",
    "        else:\n",
    "            Q[tuple(S[i]), 1] = Q1_final[i]\n",
    "    return Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dba607c2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "gammas = [0.95, 0.99, 1]\n",
    "N_Qiterations = 2\n",
    "k = 0\n",
    "gamma = 0.95\n",
    "\n",
    "\n",
    "def NFQ_it(gamma, N_Qiterations):\n",
    "    input0, input1, Q0_hat, Q1_hat, next_S0, next_S1, S, done0, done1 = initialisation()\n",
    "    Q = {}\n",
    "    for i in range(N_Qiterations):\n",
    "\n",
    "        if (1+i)%5 == 0:\n",
    "            print(i+1)\n",
    "            #print(Q[i-1])\n",
    "\n",
    "        Q0_est = Q0_hat \n",
    "        Q1_est = Q1_hat\n",
    "\n",
    "        # If terminal state, target should be 1\n",
    "        y0 = calc_target(gamma, Q0_est, done0)\n",
    "        y1 = calc_target(gamma, Q1_est, done1)\n",
    "\n",
    "        # Trainig of the NNs with TD target and input are states\n",
    "        regr0 = MLPRegressor(random_state=1, max_iter=500).fit(input0, y0)\n",
    "        regr1 = MLPRegressor(random_state=1, max_iter=500).fit(input1, y1)\n",
    "\n",
    "        # Prediction of Q_value functions for each action (0, 1)\n",
    "        Q0_hat = regr0.predict(next_S0)\n",
    "        Q1_hat = regr1.predict(next_S1)\n",
    "        \n",
    "        \n",
    "        # To compute the optimal policy, I evaluate both NNs for the state vector S and retrieve the max\n",
    "        Q0_final = regr0.predict(S)\n",
    "        Q1_final = regr1.predict(S)\n",
    "        \n",
    "        Q[i + 1] = compute_max(Q0_final, Q1_final, S)\n",
    "        \n",
    "    return Q, S\n",
    "Q = {}\n",
    "for gamma in gammas:\n",
    "    Q[gamma], S = NFQ_it(gamma, N_Qiterations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fb859832",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.9504562050302854"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q[0.95][1][tuple(S[0]), 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1dc1fa6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
